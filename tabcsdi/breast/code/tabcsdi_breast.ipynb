{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breast 0 categorical feature; 10 numerical features; total 699 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cp -r /kaggle/input/uci-breast/data_breast /kaggle/working/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters and import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import pickle\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "parser= argparse.ArgumentParser(description= \"TabCSDI\")\n",
    "parser.add_argument(\"--device\", default= 'cpu', help= \"Device\")\n",
    "parser.add_argument(\"--seed\", type= int, default= 1)\n",
    "parser.add_argument(\"--testmissingratio\", type= float, default= 0.2)\n",
    "parser.add_argument(\"--nfold\", type= int, default= 5, help= \"for 5-fold test\")\n",
    "parser.add_argument(\"--unconditional\", action= \"store_true\", default= 0)\n",
    "parser.add_argument(\"--modelfolder\", type= str, default= \"\")\n",
    "parser.add_argument(\"--nsample\", type= int, default= 100)\n",
    "args= parser.parse_args([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_func(path: str, aug_rate= 1, missing_ratio= 0.1):\n",
    "    data= pd.read_csv(path, header= None).iloc[1:, 1:]\n",
    "    data.replace(\"?\", np.nan, inplace= True)\n",
    "    data_aug= pd.concat([data]* aug_rate)\n",
    "    observed_values= data_aug.values.astype(\"float32\")\n",
    "    # False, omit; True, exist observed value.\n",
    "    observed_masks= ~np.isnan(observed_values)\n",
    "    masks= observed_masks.copy()\n",
    "    # for each column, mask {missing_ratio} % of observed values.\n",
    "    for col in range(observed_values.shape[1]):\n",
    "        obs_indices= np.where(masks[:, col])[0]\n",
    "        miss_indices= np.random.choice(obs_indices, (int)(len(obs_indices)* missing_ratio), replace= False)\n",
    "        masks[miss_indices, col]= False\n",
    "    # gt_mask: 0 for missing elements and manully maksed elements\n",
    "    gt_masks= masks.reshape(observed_masks.shape)\n",
    "    # replace missing values with 0\n",
    "    observed_values= np.nan_to_num(observed_values)\n",
    "    # convert observabd_masks and gt_masks to integer type\n",
    "    observed_masks= observed_masks.astype(int)\n",
    "    gt_masks = gt_masks.astype(int)\n",
    "    # observed_values, (line_num, col_num); observed_masks, (line_num, col_num); gt_masks, (line_num, col_num).\n",
    "    # Two dimensional form data; Two dimensional mask formed by original missing values; Two dimensional mask according missing_ratio, covering observed_masks.\n",
    "    return observed_values, observed_masks, gt_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class tabular_dataset(Dataset):\n",
    "    # eval_length should be equal to attributes number.\n",
    "    def __init__(self, eval_length= 10, use_index_list= None, aug_rate= 1, missing_ratio= 0.1, seed= 0):\n",
    "        self.eval_length= eval_length\n",
    "        np.random.seed(seed)\n",
    "        dataset_path= \"./data_breast/breast-cancer-wisconsin.data\"\n",
    "        processed_data_path= (f\"./data_breast/missing_ratio-{missing_ratio}_seed-{seed}.pk\")\n",
    "        processed_data_path_norm= (f\"./data_breast/missing_ratio-{missing_ratio}_seed-{seed}_max-min_norm.pk\")\n",
    "        if not os.path.isfile(processed_data_path):\n",
    "            # self.gt_masks, False means miss value, True means condition observation.\n",
    "            # Traverse the features one by one and mask 10% of the selected features\n",
    "            self.observed_values, self.observed_masks, self.gt_masks= process_func(\n",
    "                dataset_path, aug_rate= aug_rate, missing_ratio= missing_ratio\n",
    "            )\n",
    "            with open(processed_data_path, \"wb\") as f:\n",
    "                pickle.dump([self.observed_values, self.observed_masks, self.gt_masks], f)\n",
    "            print(\"--------Dataset created--------\")\n",
    "        elif os.path.isfile(processed_data_path_norm):\n",
    "            with open(processed_data_path_norm, \"rb\") as f:\n",
    "                self.observed_values, self.observed_masks, self.gt_masks= pickle.load(f)\n",
    "            print(\"--------Normalized dataset loaded--------\")\n",
    "        if use_index_list is None:\n",
    "            self.use_index_list= np.arange(len(self.observed_values))\n",
    "        else:\n",
    "            # row index\n",
    "            self.use_index_list= use_index_list\n",
    "\n",
    "    def __getitem__(self, org_index):\n",
    "        index= self.use_index_list[org_index]\n",
    "        s= {\n",
    "            \"observed_data\": self.observed_values[index],\n",
    "            \"observed_mask\": self.observed_masks[index],\n",
    "            \"gt_mask\": self.gt_masks[index],\n",
    "            \"timepoints\": np.arange(self.eval_length), # np.arange(K), K is feature number\n",
    "        }\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.use_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataloader(seed= 1, nfold= 5, batch_size= 128, missing_ratio= 0.1):\n",
    "    dataset= tabular_dataset(missing_ratio= missing_ratio, seed= seed)\n",
    "    print(f\"Dataset size:{len(dataset)} entries\")\n",
    "    indlist= np.arange(len(dataset))\n",
    "    np.random.seed(seed+ 1)\n",
    "    np.random.shuffle(indlist)\n",
    "    tmp_ratio= 1/ nfold\n",
    "    start= (int)((nfold- 1)* len(dataset)* tmp_ratio)\n",
    "    end= (int)(nfold* len(dataset)* tmp_ratio)\n",
    "    # end 20% for test\n",
    "    test_index= indlist[start:end]\n",
    "    remain_index= np.delete(indlist, np.arange(start, end))\n",
    "    \n",
    "    np.random.shuffle(remain_index)\n",
    "    # train, 64%; valid, 16%; test, 20%.\n",
    "    num_train= (int)(len(remain_index)* 0.8)\n",
    "    train_index= remain_index[:num_train]\n",
    "    valid_index= remain_index[num_train:]\n",
    "\n",
    "    # Here we perform max-min normalization.\n",
    "    processed_data_path_norm= (f\"./data_breast/missing_ratio-{missing_ratio}_seed-{seed}_max-min_norm.pk\")    \n",
    "    # processed_data_path_norm= (f\"./drivaernet/missing_ratio-{missing_ratio}_seed-{seed}_max-min_norm.pk\")\n",
    "    if not os.path.isfile(processed_data_path_norm):\n",
    "        print(\"--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\")\n",
    "        # data transformation after train-test split.\n",
    "        col_num= dataset.observed_values.shape[1]\n",
    "        max_arr= np.zeros(col_num)\n",
    "        min_arr= np.zeros(col_num)\n",
    "        mean_arr= np.zeros(col_num)\n",
    "        for k in range(col_num):\n",
    "            # Using observed_mask to avoid counting missing values.\n",
    "            obs_ind= dataset.observed_masks[train_index, k].astype(bool)\n",
    "            temp= dataset.observed_values[train_index, k]\n",
    "            max_arr[k]= max(temp[obs_ind])\n",
    "            min_arr[k]= min(temp[obs_ind])\n",
    "        print(f\"--------------Max-value for each column {max_arr}--------------\")\n",
    "        print(f\"--------------Min-value for each column {min_arr}--------------\")\n",
    "        # Avoid dividing by 0\n",
    "        dataset.observed_values= ((dataset.observed_values- 0+ 1)/ (max_arr- 0+ 1))* dataset.observed_masks\n",
    "        with open(processed_data_path_norm, \"wb\") as f:\n",
    "            pickle.dump([dataset.observed_values, dataset.observed_masks, dataset.gt_masks], f)\n",
    "    # Create datasets and corresponding data loaders objects.\n",
    "    train_dataset= tabular_dataset(\n",
    "        use_index_list= train_index, missing_ratio= missing_ratio, seed= seed\n",
    "    )\n",
    "    train_loader= DataLoader(train_dataset, batch_size=batch_size, shuffle=1)\n",
    "    valid_dataset= tabular_dataset(\n",
    "        use_index_list= valid_index, missing_ratio= missing_ratio, seed= seed\n",
    "    )\n",
    "    valid_loader= DataLoader(valid_dataset, batch_size= batch_size, shuffle= 0)\n",
    "    test_dataset= tabular_dataset(\n",
    "        use_index_list= test_index, missing_ratio= missing_ratio, seed= seed\n",
    "    )\n",
    "    test_loader= DataLoader(test_dataset, batch_size= batch_size, shuffle= 0)\n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(valid_dataset)}\")\n",
    "    print(f\"Testing dataset size: {len(test_dataset)}\")\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader= get_dataloader(seed= 1, nfold= 5, batch_size= 128, missing_ratio= 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config= {\n",
    "    \"train\": {\n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": 0.0005\n",
    "    },\n",
    "    \"diffusion\": {\n",
    "        \"layers\": 4,\n",
    "        \"channels\": 256,\n",
    "        \"nheads\": 2,\n",
    "        \"diffusion_embedding_dim\": 128,\n",
    "        \"beta_start\": 0.0001,\n",
    "        \"beta_end\": 0.5,\n",
    "        \"num_steps\": 150,\n",
    "        \"schedule\": \"quad\",\n",
    "        \"mixed\": False\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"is_unconditional\": 0,\n",
    "        \"timeemb\": 32,\n",
    "        \"featureemb\": 32,\n",
    "        \"target_strategy\": \"random\",\n",
    "        \"mixed\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config[\"model\"][\"is_unconditional\"]= args.unconditional\n",
    "config[\"model\"][\"test_missing_ratio\"]= args.testmissingratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_torch_trans(heads= 8, layers= 1, channels= 64):\n",
    "    # Get transformer encoder_layer\n",
    "    encoder_layer= nn.TransformerEncoderLayer(\n",
    "        d_model= channels, nhead= heads, dim_feedforward= 64, activation= \"gelu\"\n",
    "    )\n",
    "    # Get TransformerEncoder object\n",
    "    return nn.TransformerEncoder(encoder_layer, num_layers= layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Conv1d_with_init(in_channels, out_channels, kernel_size):\n",
    "    # Get Conv1d layer\n",
    "    layer= nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "    # Weight initialization\n",
    "    nn.init.kaiming_normal_(layer.weight)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DiffusionEmbedding(nn.Module):\n",
    "    # Get timestep embedding\n",
    "    def __init__(self, num_steps, embedding_dim= 128, projection_dim= None):\n",
    "        super().__init__()\n",
    "        if projection_dim is None:\n",
    "            projection_dim= embedding_dim\n",
    "        # The benefits of buffering:\n",
    "        # 1. Avoid including parameters that do not require optimization in the trainable parameters of the model\n",
    "        # 2. Efficient storage and access\n",
    "        # 3. Clearly distinguish between model parameters and auxiliary data\n",
    "        self.register_buffer(\"embedding\", self._build_embedding(num_steps, embedding_dim/ 2), persistent= False)\n",
    "        self.projection1= nn.Linear(embedding_dim, projection_dim)\n",
    "        self.projection2= nn.Linear(projection_dim, projection_dim)\n",
    "\n",
    "    def forward(self, diffusion_step):\n",
    "        x= self.embedding[diffusion_step]\n",
    "        x= self.projection1(x)\n",
    "        x= F.silu(x)\n",
    "        x= self.projection2(x)\n",
    "        x= F.silu(x)\n",
    "        return x\n",
    "\n",
    "    # t_embedding(t). The embedding dimension is 128 in total for every time step t.\n",
    "    def _build_embedding(self, num_steps, dim= 64):\n",
    "        steps= torch.arange(num_steps).unsqueeze(1)  # (T,1)\n",
    "        frequencies= 10.0** (torch.arange(dim)/ (dim- 1)* 4.0).unsqueeze(0)  # (1,dim)\n",
    "        table= steps* frequencies  # (T,dim)\n",
    "        table= torch.cat([torch.sin(table), torch.cos(table)], dim= 1)  # (T,dim*2)\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, side_dim, channels, diffusion_embedding_dim, nheads):\n",
    "        super().__init__()\n",
    "        self.diffusion_projection= nn.Linear(diffusion_embedding_dim, channels)\n",
    "        self.cond_projection= Conv1d_with_init(side_dim, 2* channels, 1)\n",
    "        self.mid_projection= Conv1d_with_init(channels, 2* channels, 1)\n",
    "        self.output_projection= Conv1d_with_init(channels, 2* channels, 1)\n",
    "        # Temporal Transformer layer\n",
    "        self.time_layer= get_torch_trans(heads= nheads, layers= 1, channels= channels)\n",
    "        # Feature Transformer layer\n",
    "        self.feature_layer= get_torch_trans(heads= nheads, layers= 1, channels= channels)\n",
    "\n",
    "    def forward_time(self, y, base_shape):\n",
    "        B, channel, K, L= base_shape\n",
    "        if L== 1:\n",
    "            return y\n",
    "        y= y.reshape(B, channel, K, L).permute(0, 2, 1, 3).reshape(B* K, channel, L)\n",
    "        # input shape for transformerencoder: [seq, batch, emb]\n",
    "        y= self.time_layer(y.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "        y= y.reshape(B, K, channel, L).permute(0, 2, 1, 3).reshape(B, channel, K* L)\n",
    "        return y\n",
    "\n",
    "    def forward_feature(self, y, base_shape):\n",
    "        B, channel, K, L= base_shape\n",
    "        if K== 1:\n",
    "            return y\n",
    "        y= y.reshape(B, channel, K, L).permute(0, 3, 1, 2).reshape(B* L, channel, K)\n",
    "        y= self.feature_layer(y.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "        y= y.reshape(B, L, channel, K).permute(0, 2, 3, 1).reshape(B, channel, K* L)\n",
    "        return y\n",
    "    \n",
    "    # x, (B, channels, K, L); cond_info, (B, 49, 1, L); diffusion_emb, (B, emb_dim).\n",
    "    def forward(self, x, cond_info, diffusion_emb):\n",
    "        B, channel, K, L= x.shape\n",
    "        base_shape= x.shape\n",
    "        x= x.reshape(B, channel, K* L)\n",
    "        # Diffusion embedding processing\n",
    "        diffusion_emb= self.diffusion_projection(diffusion_emb).unsqueeze(-1)  # (B,channel,1)\n",
    "        # x, (B, channel, KL); diffusion_emb, (B, channel, 1); y, (B, channel, KL).\n",
    "        y= x+ diffusion_emb\n",
    "        # Temporal transformer\n",
    "        y= self.forward_time(y, base_shape)\n",
    "        # Feature transformer\n",
    "        y= self.forward_feature(y, base_shape)  # (B, channel, K* L)\n",
    "        # Combining conditional information\n",
    "        y= self.mid_projection(y)  # (B, 2*channel, K* L)\n",
    "        _, cond_dim, _, _= cond_info.shape\n",
    "        cond_info= cond_info.reshape(B, cond_dim, K* L)\n",
    "        cond_info= self.cond_projection(cond_info)  # (B, 2* channel, K* L)\n",
    "        y= y+ cond_info\n",
    "        # Gated\n",
    "        gate, filter= torch.chunk(y, 2, dim= 1)\n",
    "        y= torch.sigmoid(gate)* torch.tanh(filter) # y, (B, channel, KL)\n",
    "        y= self.output_projection(y)\n",
    "        # Residual\n",
    "        residual, skip= torch.chunk(y, 2, dim= 1)\n",
    "        x= x.reshape(base_shape)\n",
    "        residual= residual.reshape(base_shape)\n",
    "        skip= skip.reshape(base_shape)\n",
    "        return (x+ residual)/ math.sqrt(2.0), skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class diff_CSDI(nn.Module):\n",
    "    # when train.\n",
    "    # inputdim= 2, cond_observed data, unmasked parts in the non-test datset; noisy data, masked parts in the non-test dataset.\n",
    "    def __init__(self, config, inputdim= 2):\n",
    "        super().__init__()\n",
    "        self.config= config\n",
    "        self.channels= config[\"channels\"]\n",
    "        self.diffusion_embedding= DiffusionEmbedding(\n",
    "            num_steps= config[\"num_steps\"],\n",
    "            embedding_dim= config[\"diffusion_embedding_dim\"] # 128\n",
    "        )\n",
    "        # token_emb_dim, 1\n",
    "        self.token_emb_dim= config[\"token_emb_dim\"] if config[\"mixed\"] else 1\n",
    "        # inputdim, 2\n",
    "        inputdim= 2* self.token_emb_dim\n",
    "        self.input_projection= Conv1d_with_init(inputdim, self.channels, 1)\n",
    "        self.output_projection1= Conv1d_with_init(self.channels, self.channels, 1)\n",
    "        self.output_projection2= Conv1d_with_init(self.channels, self.token_emb_dim, 1)\n",
    "        nn.init.zeros_(self.output_projection2.weight)\n",
    "        self.residual_layers= nn.ModuleList(\n",
    "            [\n",
    "                ResidualBlock(\n",
    "                    side_dim= config[\"side_dim\"],\n",
    "                    channels= self.channels,\n",
    "                    diffusion_embedding_dim= config[\"diffusion_embedding_dim\"],\n",
    "                    nheads= config[\"nheads\"],\n",
    "                )\n",
    "                for _ in range(config[\"layers\"])\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    # total_input, (B, 2, 1, L); side_info, (B, 49, 1, L); t, (B, ).    \n",
    "    def forward(self, x, cond_info, diffusion_step):\n",
    "        B, inputdim, K, L= x.shape\n",
    "        x= x.reshape(B, inputdim, K* L)\n",
    "        x= self.input_projection(x)\n",
    "        x= F.relu(x)\n",
    "        x= x.reshape(B, self.channels, K, L)\n",
    "        diffusion_emb= self.diffusion_embedding(diffusion_step)\n",
    "        skip= []\n",
    "        for layer in self.residual_layers:\n",
    "            x, skip_connection= layer(x, cond_info, diffusion_emb)\n",
    "            skip.append(skip_connection)\n",
    "        x= torch.sum(torch.stack(skip), dim= 0)/ math.sqrt(len(self.residual_layers))\n",
    "        x= x.reshape(B, self.channels, K* L)\n",
    "        x= self.output_projection1(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.output_projection2(x)\n",
    "        if self.config[\"mixed\"]:\n",
    "            x= x.permute(0, 2, 1)\n",
    "            x= x.reshape(B, K, L* self.token_emb_dim)\n",
    "        else:\n",
    "            x= x.reshape(B, K, L)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CSDI_base(nn.Module):\n",
    "    # Diffusion model for missing value imputation and prediction task\n",
    "    def __init__(self, target_dim, config, device):\n",
    "        super().__init__()\n",
    "        self.device= device\n",
    "        self.target_dim= target_dim\n",
    "        self.emb_time_dim= config[\"model\"][\"timeemb\"]\n",
    "        self.emb_feature_dim= config[\"model\"][\"featureemb\"]\n",
    "        self.is_unconditional= config[\"model\"][\"is_unconditional\"]\n",
    "        self.target_strategy= config[\"model\"][\"target_strategy\"]\n",
    "        self.emb_total_dim= self.emb_time_dim+ self.emb_feature_dim\n",
    "        if self.is_unconditional== False:\n",
    "            self.emb_total_dim+= 1  # for conditional mask\n",
    "        self.embed_layer= nn.Embedding(num_embeddings= self.target_dim, embedding_dim= self.emb_feature_dim)\n",
    "        config_diff= config[\"diffusion\"]\n",
    "        config_diff[\"side_dim\"]= self.emb_total_dim\n",
    "        # input_dim, 2\n",
    "        input_dim= 1 if self.is_unconditional== True else 2\n",
    "        self.diffmodel= diff_CSDI(config_diff, input_dim)\n",
    "        # parameters for diffusion models\n",
    "        self.num_steps = config_diff[\"num_steps\"]\n",
    "        if config_diff[\"schedule\"] == \"quad\":\n",
    "            self.beta = (\n",
    "                np.linspace(\n",
    "                    config_diff[\"beta_start\"] ** 0.5,\n",
    "                    config_diff[\"beta_end\"] ** 0.5,\n",
    "                    self.num_steps,\n",
    "                )\n",
    "                ** 2\n",
    "            )\n",
    "        elif config_diff[\"schedule\"] == \"linear\":\n",
    "            self.beta = np.linspace(\n",
    "                config_diff[\"beta_start\"], config_diff[\"beta_end\"], self.num_steps\n",
    "            )\n",
    "\n",
    "        self.alpha_hat = 1 - self.beta\n",
    "        self.alpha = np.cumprod(self.alpha_hat)\n",
    "        self.alpha_torch = (\n",
    "            torch.tensor(self.alpha).float().to(self.device).unsqueeze(1).unsqueeze(1)\n",
    "        )\n",
    "\n",
    "    def time_embedding(self, pos, d_model= 128):\n",
    "        pe= torch.zeros(pos.shape[0], pos.shape[1], d_model).to(self.device)\n",
    "        position= pos.unsqueeze(2)\n",
    "        div_term= 1/ torch.pow(10000.0, torch.arange(0, d_model, 2).to(self.device)/ d_model)\n",
    "        pe[:, :, 0:: 2]= torch.sin(position* div_term)\n",
    "        pe[:, :, 1:: 2]= torch.cos(position* div_term)\n",
    "        return pe\n",
    "    \n",
    "    def get_randmask(self, observed_mask):\n",
    "        # Randomly mask 20% of each record in observabd_mask.\n",
    "        rand_for_mask= torch.rand_like(observed_mask)* observed_mask\n",
    "        rand_for_mask= rand_for_mask.reshape(len(rand_for_mask), -1)\n",
    "        # for each record\n",
    "        for i in range(len(observed_mask)):\n",
    "            sample_ratio= np.random.rand()  # missing ratio\n",
    "            num_observed= observed_mask[i].sum().item() # observed number\n",
    "            num_masked= round(num_observed* sample_ratio) # masked number\n",
    "            rand_for_mask[i][rand_for_mask[i].topk(num_masked).indices]= -1\n",
    "        # cond_mask 1, pred item; 0, cond item.\n",
    "        cond_mask= (rand_for_mask> 0).reshape(observed_mask.shape).float() # random mask 10% for each record\n",
    "        return cond_mask\n",
    "\n",
    "    def get_side_info(self, observed_tp, cond_mask):\n",
    "        B, K, L= cond_mask.shape\n",
    "        time_embed= self.time_embedding(observed_tp, self.emb_time_dim)\n",
    "        time_embed= time_embed.unsqueeze(2).expand(-1, -1, K, -1)\n",
    "        feature_embed= self.embed_layer(torch.arange(self.target_dim).to(self.device))\n",
    "        feature_embed= feature_embed.unsqueeze(0).unsqueeze(2).expand(B, -1, K, -1)\n",
    "        side_info= time_embed\n",
    "        side_info= torch.cat([0* time_embed, feature_embed], dim=-1)  # (B,L,K,*)\n",
    "        side_info= side_info.permute(0, 3, 2, 1)  # (B,*,K,L)\n",
    "        if self.is_unconditional== False:\n",
    "            side_mask= cond_mask.unsqueeze(1)  # (B,1,K,L)\n",
    "            side_info= torch.cat([side_info, side_mask], dim= 1)\n",
    "        return side_info\n",
    "\n",
    "    def calc_loss_valid(\n",
    "        self, observed_data, cond_mask, observed_mask, side_info, is_train\n",
    "    ):\n",
    "        loss_sum = 0\n",
    "        # In validation, perform T steps forward and backward.\n",
    "        for t in range(self.num_steps):\n",
    "            loss = self.calc_loss(\n",
    "                observed_data, cond_mask, observed_mask, side_info, is_train, set_t=t\n",
    "            )\n",
    "            loss_sum += loss.detach()\n",
    "        return loss_sum / self.num_steps\n",
    "\n",
    "    def calc_loss(self, observed_data, cond_mask, observed_mask, side_info, is_train, set_t= -1):\n",
    "        B, K, L= observed_data.shape\n",
    "        if is_train!= 1:  # for validation\n",
    "            t= (torch.ones(B)* set_t).long().to(self.device)\n",
    "        else:\n",
    "            t= torch.randint(0, self.num_steps, [B]).to(self.device)\n",
    "        current_alpha= self.alpha_torch[t]\n",
    "        noise= torch.randn_like(observed_data)\n",
    "        noisy_data= (current_alpha** 0.5)* observed_data+ (1.0- current_alpha)** 0.5* noise\n",
    "        total_input= self.set_input_to_diffmodel(noisy_data, observed_data, cond_mask)\n",
    "        predicted= self.diffmodel(total_input, side_info, t)  # (B, K, L)\n",
    "        target_mask = observed_mask - cond_mask\n",
    "        residual = (noise - predicted) * target_mask\n",
    "        num_eval = target_mask.sum()\n",
    "        loss = (residual**2).sum() / (num_eval if num_eval > 0 else 1)\n",
    "        return loss\n",
    "    \n",
    "    # input of diffusion model\n",
    "    def set_input_to_diffmodel(self, noisy_data, observed_data, cond_mask):\n",
    "        if self.is_unconditional== True:\n",
    "            total_input= noisy_data.unsqueeze(1)  # (B,1,K,L)\n",
    "        else:\n",
    "            cond_obs= (cond_mask* observed_data).unsqueeze(1)\n",
    "            noisy_target= ((1- cond_mask)* noisy_data).unsqueeze(1)\n",
    "            total_input= torch.cat([cond_obs, noisy_target], dim= 1)  # (B,2,K,L)\n",
    "        return total_input\n",
    "\n",
    "    def impute(self, observed_data, cond_mask, side_info, n_samples):\n",
    "        B, K, L= observed_data.shape\n",
    "        imputed_samples= torch.zeros(B, n_samples, K, L).to(self.device)\n",
    "        for i in range(n_samples):\n",
    "            # generate noisy observation for unconditional model\n",
    "            if self.is_unconditional == True:\n",
    "                noisy_obs = observed_data\n",
    "                noisy_cond_history = []\n",
    "                # perform T steps forward\n",
    "                for t in range(self.num_steps):\n",
    "                    noise = torch.randn_like(noisy_obs)\n",
    "                    noisy_obs = (self.alpha_hat[t] ** 0.5) * noisy_obs + self.beta[t] ** 0.5 * noise\n",
    "                    noisy_cond_history.append(noisy_obs * cond_mask)\n",
    "            current_sample = torch.randn_like(observed_data)\n",
    "            # perform T steps backward\n",
    "            for t in range(self.num_steps- 1, -1, -1):\n",
    "                if self.is_unconditional == True:\n",
    "                    diff_input = (cond_mask * noisy_cond_history[t]+ (1.0 - cond_mask) * current_sample)\n",
    "                    diff_input = diff_input.unsqueeze(1)  # (B,1,K,L)\n",
    "                else:\n",
    "                    cond_obs = (cond_mask * observed_data).unsqueeze(1)\n",
    "                    noisy_target = ((1 - cond_mask) * current_sample).unsqueeze(1)\n",
    "                    diff_input = torch.cat([cond_obs, noisy_target], dim=1)  # (B,2,K,L)\n",
    "                predicted = self.diffmodel(diff_input, side_info, torch.tensor([t]).to(self.device))\n",
    "                coeff1 = 1 / self.alpha_hat[t] ** 0.5\n",
    "                coeff2 = (1 - self.alpha_hat[t]) / (1 - self.alpha[t]) ** 0.5\n",
    "                current_sample = coeff1 * (current_sample - coeff2 * predicted)\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(current_sample)\n",
    "                    sigma = ((1.0 - self.alpha[t - 1]) / (1.0 - self.alpha[t]) * self.beta[t]) ** 0.5\n",
    "                    current_sample += sigma * noise\n",
    "            imputed_samples[:, i] = current_sample.detach()\n",
    "        return imputed_samples\n",
    "\n",
    "    def forward(self, batch, is_train= 1):\n",
    "        (observed_data, observed_mask, observed_tp, gt_mask, for_pattern_mask, _,)= self.process_data(batch)\n",
    "        # In testing, using `gt_mask` (generated with fixed missing rate).\n",
    "        if is_train== 0:\n",
    "            cond_mask= gt_mask\n",
    "        # In training, generate random mask\n",
    "        else:\n",
    "            cond_mask= self.get_randmask(observed_mask)\n",
    "        # side information including time embedding+ feature embedding+ cond_mask\n",
    "        side_info= self.get_side_info(observed_tp, cond_mask)\n",
    "        # train, mse\n",
    "        loss_func= self.calc_loss if is_train == 1 else self.calc_loss_valid\n",
    "        return loss_func(observed_data, cond_mask, observed_mask, side_info, is_train)\n",
    "\n",
    "    def evaluate(self, batch, n_samples):\n",
    "        (observed_data, observed_mask, observed_tp, gt_mask, _, cut_length, )= self.process_data(batch)\n",
    "        with torch.no_grad():\n",
    "            cond_mask= gt_mask\n",
    "            target_mask= observed_mask- cond_mask\n",
    "            side_info= self.get_side_info(observed_tp, cond_mask)\n",
    "            # imputate.\n",
    "            # n_samples, 100, Control the sampling frequency of the model during evaluation; stability assessment.\n",
    "            samples= self.impute(observed_data, cond_mask, side_info, n_samples)\n",
    "        return samples, observed_data, target_mask, observed_mask, observed_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TabCSDI(CSDI_base):\n",
    "    def __init__(self, config, device, target_dim= 10):\n",
    "        super(TabCSDI, self).__init__(target_dim, config, device)\n",
    "\n",
    "    def process_data(self, batch):\n",
    "        # Insert K=1 axis. All mask now with shape (B, 1, L).\n",
    "        observed_data= batch[\"observed_data\"][:, np.newaxis, :] # (B, 1, L)\n",
    "        observed_data= observed_data.to(self.device).float()\n",
    "        observed_mask= batch[\"observed_mask\"][:, np.newaxis, :] # (B, 1, L)\n",
    "        observed_mask= observed_mask.to(self.device).float()\n",
    "        observed_tp= batch[\"timepoints\"].to(self.device).float() # (B, 10)\n",
    "        gt_mask= batch[\"gt_mask\"][:, np.newaxis, :]\n",
    "        gt_mask= gt_mask.to(self.device).float() # (B, 1, L)\n",
    "        cut_length= torch.zeros(len(observed_data)).long().to(self.device) # (B, )\n",
    "        for_pattern_mask= observed_mask # (B, 1, L)\n",
    "        return (observed_data, observed_mask, observed_tp, gt_mask, for_pattern_mask, cut_length,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, config, train_loader, valid_loader= None, valid_epoch_interval= 20, foldername= \"\",):\n",
    "    # Control random seed in the current script.\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    optimizer= Adam(model.parameters(), lr= config[\"lr\"], weight_decay= 1e-6)\n",
    "    if foldername!= \"\":\n",
    "        output_path= foldername+ \"/model.pth\"\n",
    "    p0= int(0.25* config[\"epochs\"])\n",
    "    p1= int(0.5* config[\"epochs\"])\n",
    "    p2= int(0.75* config[\"epochs\"])\n",
    "    p3= int(0.9* config[\"epochs\"])\n",
    "    # When the number of training rounds reaches p0, p1, p2, and p3, the learning rate will decay.\n",
    "    lr_scheduler= torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones= [p0, p1, p2, p3], gamma= 0.1)\n",
    "    history = {'train_loss':[], 'val_loss':[], 'val_rmse':[], 'val_mae':[]}\n",
    "    best_valid_rmse= 1e10\n",
    "    for epoch_no in range(config[\"epochs\"]):\n",
    "        train_avg_loss= 0\n",
    "        model.train()\n",
    "        # The minimum and maximum time intervals for progress bar updates have been specified.\n",
    "        with tqdm(train_loader, mininterval= 5.0, maxinterval= 50.0) as it:\n",
    "            for batch_no, train_batch in enumerate(it, start= 1):\n",
    "                optimizer.zero_grad()\n",
    "                # The forward method returns loss.\n",
    "                loss= model(train_batch)\n",
    "                loss.backward()\n",
    "                train_avg_loss+= loss.item()\n",
    "                optimizer.step()\n",
    "                it.set_postfix(\n",
    "                    ordered_dict= {\"avg_epoch_loss\": train_avg_loss/ batch_no, \"epoch\": epoch_no,},\n",
    "                    refresh= False,\n",
    "                )\n",
    "            # step each epoch.\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        if valid_loader is not None and (epoch_no+ 1)% valid_epoch_interval== 0:\n",
    "            history['train_loss'].append(train_avg_loss/ batch_no)\n",
    "            print(\"Start validation\")\n",
    "            model.eval()\n",
    "            avg_loss_valid= 0\n",
    "            # some initial settings\n",
    "            val_nsample= 10\n",
    "            val_scaler= 1\n",
    "            mse_total= 0\n",
    "            mae_total= 0\n",
    "            evalpoints_total= 0\n",
    "            with torch.no_grad():\n",
    "                with tqdm(valid_loader, mininterval= 5.0, maxinterval= 50.0) as it:\n",
    "                    for batch_no, valid_batch in enumerate(it, start= 1):\n",
    "                        loss= model(train_batch)\n",
    "                        avg_loss_valid+= loss.item()\n",
    "                        output= model.evaluate(valid_batch, val_nsample)\n",
    "                        # `eval_points` is `target_mask`. `observed_time` is `observed_tp`(10)\n",
    "                        # `c_target` is `observed_data`\n",
    "                        (samples, c_target, eval_points, observed_points, observed_time,)= output\n",
    "                        samples= samples.permute(0, 1, 3, 2)\n",
    "                        c_target= c_target.permute(0, 2, 1)\n",
    "                        eval_points= eval_points.permute(0, 2, 1)\n",
    "                        observed_points= observed_points.permute(0, 2, 1)\n",
    "                        samples_median= samples.median(dim= 1)\n",
    "                        mse_current= (((samples_median.values- c_target)* eval_points)** 2)* (val_scaler** 2)\n",
    "                        mae_current= (torch.abs((samples_median.values- c_target)* eval_points))* val_scaler\n",
    "                        mae_total+= torch.sum(mae_current, dim= 0)\n",
    "                        mse_total+= torch.sum(mse_current, dim= 0)\n",
    "                        evalpoints_total+= torch.sum(eval_points, dim= 0)\n",
    "                        it.set_postfix(\n",
    "                            ordered_dict= {\n",
    "                                \"rmse_total\": torch.mean(torch.sqrt(torch.div(mse_total, evalpoints_total))).item(),\n",
    "                                \"batch_no\": batch_no,\n",
    "                            },\n",
    "                            refresh= True,\n",
    "                        )\n",
    "                    history['val_rmse'].append(torch.mean(torch.sqrt(torch.div(mse_total, evalpoints_total))).item())\n",
    "                    history['val_mae'].append(torch.mean(torch.div(mae_total, evalpoints_total)).item())\n",
    "                    history['val_loss'].append(avg_loss_valid/ batch_no)\n",
    "                    # save model\n",
    "                    if best_valid_rmse> torch.mean(torch.sqrt(torch.div(mse_total, evalpoints_total))).item() and foldername!= \"\":\n",
    "                        torch.save(model.state_dict(), output_path)\n",
    "    # Use folloing code for saving training history.\n",
    "    with open(foldername+'/saved_history.pkl', 'wb') as f:\n",
    "        pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, nsample= 100, scaler= 1, mean_scaler=0, foldername=\"\"):\n",
    "    # Control random seed in the current script.\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        mse_total = 0\n",
    "        mae_total = 0\n",
    "        evalpoints_total = 0\n",
    "        all_target = []\n",
    "        all_observed_point = []\n",
    "        all_observed_time = []\n",
    "        all_evalpoint = []\n",
    "        all_generated_samples = []\n",
    "        with tqdm(test_loader, mininterval=5.0, maxinterval=50.0) as it:\n",
    "            for batch_no, test_batch in enumerate(it, start= 1):\n",
    "                output = model.evaluate(test_batch, nsample)\n",
    "                # samples, (B, 100, 1, L); observed_data, (B, 1, L); target_mask, (B, 1, L).\n",
    "                # observed_mask, (B, 1, L); observed_tp, (B, 10).\n",
    "                samples, c_target, eval_points, observed_points, observed_time = output\n",
    "                samples = samples.permute(0, 1, 3, 2)  # (B,nsample,L,K)\n",
    "                c_target = c_target.permute(0, 2, 1)  # (B,L,K)\n",
    "                eval_points = eval_points.permute(0, 2, 1)\n",
    "                observed_points = observed_points.permute(0, 2, 1)\n",
    "                # take the median from samples.\n",
    "                # samples_median, (B, L, 1).\n",
    "                samples_median = samples.median(dim=1)\n",
    "                all_target.append(c_target)\n",
    "                all_evalpoint.append(eval_points)\n",
    "                all_observed_point.append(observed_points)\n",
    "                all_observed_time.append(observed_time)\n",
    "                all_generated_samples.append(samples)\n",
    "                mse_current = (((samples_median.values - c_target) * eval_points) ** 2) * (scaler**2)\n",
    "                mae_current = (torch.abs((samples_median.values - c_target) * eval_points)) * scaler\n",
    "                mse_total += torch.sum(mse_current, dim=0)\n",
    "                mae_total += torch.sum(mae_current, dim=0)\n",
    "                evalpoints_total += torch.sum(eval_points, dim=0)\n",
    "                it.set_postfix(\n",
    "                    ordered_dict={\n",
    "                        \"rmse_total\": torch.mean(\n",
    "                            torch.sqrt(torch.div(mse_total, evalpoints_total))\n",
    "                        ).item(),\n",
    "                        \"batch_no\": batch_no,\n",
    "                    },\n",
    "                    refresh=True,\n",
    "                )\n",
    "\n",
    "            with open(foldername + \"/result_nsample\" + str(nsample) + \".pk\", \"wb\") as f:\n",
    "                pickle.dump([torch.mean(torch.sqrt(torch.div(mse_total, evalpoints_total))).item(),], f, )\n",
    "            print(\"RMSE:\", torch.mean(torch.sqrt(torch.div(mse_total, evalpoints_total))).item(), )\n",
    "            \n",
    "        # Use following code for saving generated results.\n",
    "        with open(foldername + \"/generated_outputs_nsample\" + str(nsample) + \".pk\", \"wb\") as f:\n",
    "            all_target = torch.cat(all_target, dim=0)\n",
    "            all_evalpoint = torch.cat(all_evalpoint, dim=0)\n",
    "            all_observed_point = torch.cat(all_observed_point, dim=0)\n",
    "            all_observed_time = torch.cat(all_observed_time, dim=0)\n",
    "            all_generated_samples = torch.cat(all_generated_samples, dim=0)\n",
    "            pickle.dump(\n",
    "                [\n",
    "                    all_generated_samples,\n",
    "                    all_target,\n",
    "                    all_evalpoint,\n",
    "                    all_observed_point,\n",
    "                    all_observed_time,\n",
    "                    scaler,\n",
    "                    mean_scaler,\n",
    "                ],\n",
    "                f,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args.device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model= TabCSDI(config, args.device, target_dim= 10).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create folder\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "foldername = \"./save/breast_fold\" + str(args.nfold) + \"_\" + current_time + \"/\"\n",
    "print(\"model folder:\", foldername)\n",
    "os.makedirs(foldername, exist_ok=True)\n",
    "with open(foldername + \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "# Train model\n",
    "if args.modelfolder== \"\":\n",
    "    train(\n",
    "        model,\n",
    "        config[\"train\"],\n",
    "        train_loader,\n",
    "        valid_loader= valid_loader,\n",
    "        foldername= foldername,\n",
    "    )\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"./save/\" + args.modelfolder + \"/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"---------------Start testing---------------\")\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/save/breast_fold5_20241108_123916/model.pth\"))\n",
    "evaluate(model, test_loader, nsample= 10, scaler= 1, foldername= foldername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MeanNet(nn.Module):\n",
    "    def __init__(self, feat_mean_val):\n",
    "        super().__init__()\n",
    "        self.fmv= feat_mean_val\n",
    "    def forward(self, x):\n",
    "        return self.fmv[torch.where(x['gt_mask']== 0)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset= tabular_dataset(missing_ratio= 0.1, seed= args.seed)\n",
    "meannet= MeanNet(torch.from_numpy((dataset.observed_values* dataset.gt_masks).mean(axis= 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mse_total= 0\n",
    "mae_total= 0\n",
    "evalpoints_total= 0\n",
    "for batch_id, batch_test in enumerate(test_loader):\n",
    "    label= batch_test['observed_data'][~batch_test['gt_mask'].to(torch.bool)]\n",
    "    pred= meannet(batch_test)\n",
    "    mse_current= ((pred- label) ** 2)\n",
    "    mae_current= torch.abs(pred- label)\n",
    "    mse_total+= torch.sum(mse_current, dim= 0)\n",
    "    mae_total+= torch.sum(mae_current, dim= 0)\n",
    "    evalpoints_total+= (batch_test['gt_mask']== 0).sum()\n",
    "print(f'rmse:{torch.sqrt(mse_total/ evalpoints_total)}\\nmae:{mae_total/ evalpoints_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "with open(f'./{foldername}/saved_history.pkl', 'rb') as f:\n",
    "    history= pickle.load(f)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history['train_loss'], label= 'Train Loss')\n",
    "plt.plot(history['val_loss'], label= 'Valid Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title('Training and Validation Loss over each 20 Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('Training and Validation Loss.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "with open(f'./{foldername}/saved_history.pkl', 'rb') as f:\n",
    "    history= pickle.load(f)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history['val_rmse'], label= 'Valid RMSE')\n",
    "plt.plot(history['val_mae'], label= 'Valid MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Validation RMSE and MAE over each 20 Epochs')\n",
    "plt.legend()\n",
    "plt.savefig('Validation RMSE and MAE.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5922676,
     "sourceId": 9688210,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6006594,
     "sourceId": 9800709,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6006700,
     "sourceId": 9800848,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6016311,
     "sourceId": 9813494,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
